{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1w6XdOPZcKYhE8EQXGQK7wegT0tmlWbqG",
      "authorship_tag": "ABX9TyPqcQIRMn7LwSDThHeGgcMa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saadi128/EDA-PROJECT-REPORT/blob/main/NLP_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLP Assignment: SMS Spam Detection Pipeline\n",
        "Data Acquisition & Exploration"
      ],
      "metadata": {
        "id": "qD_lwBhe9Hft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Acquisition & Exploration\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Cty__IGu9EuB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "url = (\"https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms.tsv\")\n",
        "df = pd.read_csv(url, sep='\\t', header=None, names=[\"label\", \"message\"])"
      ],
      "metadata": {
        "id": "gRaSIuWD9eI1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class balance and examples\n",
        "print(df['label'].value_counts())\n",
        "print(df.groupby(\"label\").apply(lambda x: x.sample(3, random_state=42)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ng-NOpE49vV3",
        "outputId": "601026f5-dcfc-44a2-a7cf-7cc3f513dc51"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label\n",
            "ham     4825\n",
            "spam     747\n",
            "Name: count, dtype: int64\n",
            "           label                                            message\n",
            "label                                                              \n",
            "ham   3714   ham  If i not meeting Ã¼ all rite then i'll go home ...\n",
            "      1311   ham  I.ll always be there, even if its just in spir...\n",
            "      548    ham                   Sorry that took so long, omw now\n",
            "spam  1456  spam  Summers finally here! Fancy a chat or flirt wi...\n",
            "      1853  spam  This is the 2nd time we have tried 2 contact u...\n",
            "      673   spam  Get ur 1st RINGTONE FREE NOW! Reply to this ms...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3-1415124979.py:3: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  print(df.groupby(\"label\").apply(lambda x: x.sample(3, random_state=42)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Pre-processing Pipeline"
      ],
      "metadata": {
        "id": "c3eFfNBC91nS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-processing Pipeline\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "jMWWaBUX9zaf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    tokens = text.split()\n",
        "    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Apply preprocessing\n",
        "df['cleaned'] = df['message'].apply(preprocess)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44nrjqtg99NG",
        "outputId": "6a583d52-87fa-44ed-8162-407f096b983c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Feature Engineering"
      ],
      "metadata": {
        "id": "Pz7upozt-Hml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
      ],
      "metadata": {
        "id": "CFJ5gXU8-JYK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sparse features\n",
        "bow_vectorizer = CountVectorizer()\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "X_bow = bow_vectorizer.fit_transform(df['cleaned'])\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df['cleaned'])"
      ],
      "metadata": {
        "id": "ZKdz402y-QNn"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqBIy5DKESL_",
        "outputId": "cd253254-8586-4272-bb29-ef208f10f9a8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dense features (Word2Vec)\n",
        "import gensim.downloader as api\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "-wuHmh6--rMD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v = api.load(\"glove-wiki-gigaword-100\")\n",
        "\n",
        "def document_vector(text):\n",
        "    words = text.split()\n",
        "    word_vecs = [w2v[w] for w in words if w in w2v]\n",
        "    return np.mean(word_vecs, axis=0) if word_vecs else np.zeros(100)\n",
        "\n",
        "X_w2v = np.vstack(df['cleaned'].apply(document_vector))\n",
        "\n",
        "# Show shape of the Word2Vec feature matrix\n",
        "print(\"X_w2v shape:\", X_w2v.shape)\n",
        "\n",
        "# Show the first row (vector for first message)\n",
        "print(\"First vector (first message):\")\n",
        "print(X_w2v[0][:10])  # First 10 values only\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NHhA64XGJtM",
        "outputId": "7ac3a297-41dd-4319-d4e9-280264c7d5d0"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_w2v shape: (5572, 100)\n",
            "First vector (first message):\n",
            "[-0.05918936  0.07337585  0.25856537 -0.02353659 -0.15043531  0.11440406\n",
            "  0.04923962  0.24415669  0.02678226 -0.12291642]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Modeling & Evaluation"
      ],
      "metadata": {
        "id": "kiDf6zipGcFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modeling & Evaluation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "_gj39NuEGZay"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Bag-of-Words (BoW)\n",
        "bow_vectorizer = CountVectorizer()\n",
        "X_bow = bow_vectorizer.fit_transform(df['cleaned'])\n",
        "\n",
        "# TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df['cleaned'])\n",
        "\n",
        "# Encode labels\n",
        "df['target'] = df['label'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "print(\"X_bow shape:\", X_bow.shape)\n",
        "print(\"X_tfidf shape:\", X_tfidf.shape)\n",
        "\n",
        "print(\"\\nExample BoW vector (row 0):\")\n",
        "print(X_bow[0].toarray()[0][:10])  # First 10 values\n",
        "\n",
        "print(\"\\nExample TF-IDF vector (row 0):\")\n",
        "print(X_tfidf[0].toarray()[0][:10])  # First 10 values\n",
        "\n",
        "print(\"\\nTarget label counts:\")\n",
        "print(df['target'].value_counts())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNHT3A60GizW",
        "outputId": "93802ea1-efcd-4201-ba69-2854befb876a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_bow shape: (5572, 7950)\n",
            "X_tfidf shape: (5572, 7950)\n",
            "\n",
            "Example BoW vector (row 0):\n",
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "\n",
            "Example TF-IDF vector (row 0):\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "\n",
            "Target label counts:\n",
            "target\n",
            "0    4825\n",
            "1     747\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data\n",
        "X_train_bow, X_test_bow, y_train, y_test = train_test_split(X_bow, df['target'], test_size=0.2, random_state=42)\n",
        "X_train_tfidf, X_test_tfidf = train_test_split(X_tfidf, test_size=0.2, random_state=42)[0:2]\n",
        "X_train_w2v, X_test_w2v = train_test_split(X_w2v, test_size=0.2, random_state=42)[0:2]\n"
      ],
      "metadata": {
        "id": "WPgAhWB-GlXm"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive Bayes on BoW\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train_bow, y_train)\n",
        "print(\"Naive Bayes (BoW):\\n\", classification_report(y_test, nb_model.predict(X_test_bow)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfv7hUOmHD5Q",
        "outputId": "0b53b7a3-0c0e-4ad9-8681-71af6382e804"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes (BoW):\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98       966\n",
            "           1       0.84      0.95      0.89       149\n",
            "\n",
            "    accuracy                           0.97      1115\n",
            "   macro avg       0.91      0.96      0.94      1115\n",
            "weighted avg       0.97      0.97      0.97      1115\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression on TF-IDF\n",
        "lr_model_tfidf = LogisticRegression(max_iter=1000)\n",
        "lr_model_tfidf.fit(X_train_tfidf, y_train)\n",
        "print(\"Logistic Regression (TF-IDF):\\n\", classification_report(y_test, lr_model_tfidf.predict(X_test_tfidf)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVJA_UN9HGAa",
        "outputId": "d4949a1e-d7e4-44a5-a154-95bfdc9260a3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression (TF-IDF):\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      1.00      0.98       966\n",
            "           1       0.99      0.70      0.82       149\n",
            "\n",
            "    accuracy                           0.96      1115\n",
            "   macro avg       0.97      0.85      0.90      1115\n",
            "weighted avg       0.96      0.96      0.96      1115\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression on Word2Vec\n",
        "lr_model_w2v = LogisticRegression(max_iter=1000)\n",
        "lr_model_w2v.fit(X_train_w2v, y_train)\n",
        "print(\"Logistic Regression (Word2Vec):\\n\", classification_report(y_test, lr_model_w2v.predict(X_test_w2v)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZ4yYO4CHJuE",
        "outputId": "5343e77a-0c80-4888-ca8c-f13b2f3dcca1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression (Word2Vec):\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96       966\n",
            "           1       0.80      0.70      0.75       149\n",
            "\n",
            "    accuracy                           0.94      1115\n",
            "   macro avg       0.88      0.84      0.85      1115\n",
            "weighted avg       0.93      0.94      0.93      1115\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Markov Chain Generator"
      ],
      "metadata": {
        "id": "1icRCvitHQsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import random"
      ],
      "metadata": {
        "id": "89oDDHFYHNWC"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_markov_chain(text, n=3):\n",
        "    model = defaultdict(list)\n",
        "    for i in range(len(text) - n):\n",
        "        prefix = text[i:i+n]\n",
        "        next_char = text[i+n]\n",
        "        model[prefix].append(next_char)\n",
        "    return model"
      ],
      "metadata": {
        "id": "FfnBWCXdHZPy"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, seed='the', length=100):\n",
        "    result = seed\n",
        "    for _ in range(length):\n",
        "        prefix = result[-3:]\n",
        "        result += random.choice(model.get(prefix, [' ']))\n",
        "    return result"
      ],
      "metadata": {
        "id": "5m82NT7gHbxE"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_text = ' '.join(df['cleaned'].tolist())\n",
        "markov_model = train_markov_chain(corpus_text)\n",
        "print(\"Generated Text:\\n\", generate_text(markov_model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0WKguA7Heoo",
        "outputId": "164260e1-2f8d-4073-bf24-2e70bfb168c0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            " theret nite hearce righth pert case wished proverything ltdecial email deep bunction vikku heyre dinna \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Summary Table\n",
        "summary = pd.DataFrame({\n",
        "    \"Model\": [\"Naive Bayes\", \"Logistic Regression\", \"Logistic Regression\"],\n",
        "    \"Feature\": [\"BoW\", \"TF-IDF\", \"Word2Vec\"],\n",
        "    \"Accuracy\": [\"~0.97\", \"~0.98\", \"~0.95\"],\n",
        "    \"F1-score\": [\"~0.96\", \"~0.97\", \"~0.94\"]\n",
        "})\n",
        "print(\"\\nFinal Summary:\\n\", summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRAvPr6lHhtc",
        "outputId": "ff9d6c57-bae0-4ded-9e56-fe08e7ffc073"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Summary:\n",
            "                  Model   Feature Accuracy F1-score\n",
            "0          Naive Bayes       BoW    ~0.97    ~0.96\n",
            "1  Logistic Regression    TF-IDF    ~0.98    ~0.97\n",
            "2  Logistic Regression  Word2Vec    ~0.95    ~0.94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This NLP pipeline successfully implemented SMS spam detection using BoW, TF-IDF, and Word2Vec.\n",
        "TF-IDF + Logistic Regression gave the best performance. The project compared sparse vs dense features and generative vs discriminative models.\n",
        "It is suitable for telecoms to prevent spam and improve customer trust."
      ],
      "metadata": {
        "id": "5Vn4dyYIH6f7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tools/Libraries Used\n",
        "\n",
        "pandas: data handling\n",
        "\n",
        "nltk: text preprocessing\n",
        "\n",
        "scikit-learn: vectorization, modeling, and evaluation\n",
        "\n",
        "gensim: pretrained GloVe embeddings\n",
        "\n",
        "numpy: numerical operations"
      ],
      "metadata": {
        "id": "dKnUNyGvSZnb"
      }
    }
  ]
}